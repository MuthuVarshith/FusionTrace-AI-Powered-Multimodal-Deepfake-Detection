{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle authentication\n",
    "import os\n",
    "import shutil\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Make directory and move credentials\n",
    "os.makedirs(\".kaggle\", exist_ok=True)\n",
    "shutil.move(\".kaggle.json\", \".kaggle/kaggle.json\")\n",
    "\n",
    "# Set environment variable and authenticate\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.path.abspath('.kaggle')\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "print(\"Authenticated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_dataset(dataset_id, download_path):\n",
    "    \"\"\"Download and extract a Kaggle dataset\"\"\"\n",
    "    zip_path = os.path.join(download_path, f\"{dataset_id.split('/')[-1]}.zip\")\n",
    "    \n",
    "    # Create directory\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "    # Download using Kaggle CLI\n",
    "    print(f\"\\n⬇Downloading: {dataset_id}\")\n",
    "    subprocess.run(\n",
    "        [\"kaggle\", \"datasets\", \"download\", \"-d\", dataset_id, \"-p\", download_path],\n",
    "        check=True\n",
    "    )\n",
    "    print(f\"Download completed: {os.path.basename(zip_path)}\")\n",
    "    \n",
    "    # Extract with progress bar\n",
    "    print(\"Extracting contents...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        total_files = len(zip_ref.infolist())\n",
    "        for file in tqdm(zip_ref.infolist(), desc=\"Extracting\", unit=\"file\", total=total_files):\n",
    "            zip_ref.extract(file, path=download_path)\n",
    "    \n",
    "    # Remove zip file\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Dataset ready at: {download_path}\")\n",
    "\n",
    "# Download both datasets\n",
    "download_dataset(\"alaaeddineayadi/real-vs-ai-generated-faces\", \"./datasets/real_vs_ai_faces\")\n",
    "download_dataset(\"xhlulu/140k-real-and-fake-faces\", \"./datasets/140k_faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_folder_tree(start_path, prefix=\"\"):\n",
    "    \"\"\"Recursively print folder structure\"\"\"\n",
    "    items = [item for item in os.listdir(start_path) if os.path.isdir(os.path.join(start_path, item))]\n",
    "    pointers = ['├── '] * (len(items) - 1) + ['└── ']\n",
    "\n",
    "    for pointer, item in zip(pointers, items):\n",
    "        path = os.path.join(start_path, item)\n",
    "        print(prefix + pointer + item)\n",
    "        extension = '│   ' if pointer == '├── ' else '    '\n",
    "        print_folder_tree(path, prefix + extension)\n",
    "\n",
    "# Print structure of both datasets\n",
    "print(\"Folder structure of 'real_vs_ai_faces':\")\n",
    "print_folder_tree('datasets/real_vs_ai_faces')\n",
    "\n",
    "print(\"\\nFolder structure of '140k_faces':\")\n",
    "print_folder_tree('datasets/140k_faces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\"Combine and balance datasets\"\"\"\n",
    "    # Output folders\n",
    "    output_real = 'data/real'\n",
    "    output_fake = 'data/fake'\n",
    "    os.makedirs(output_real, exist_ok=True)\n",
    "    os.makedirs(output_fake, exist_ok=True)\n",
    "\n",
    "    # Source directories from both datasets\n",
    "    input_dirs = [\n",
    "        # From 140k_faces dataset\n",
    "        'datasets/140k_faces/real_vs_fake/real-vs-fake/valid/real',\n",
    "        'datasets/140k_faces/real_vs_fake/real-vs-fake/valid/fake',\n",
    "        \n",
    "        # From real_vs_ai_faces dataset\n",
    "        'datasets/real_vs_ai_faces/dataset/train/real',\n",
    "        'datasets/real_vs_ai_faces/dataset/val/real',\n",
    "        'datasets/real_vs_ai_faces/dataset/test/real',\n",
    "        'datasets/real_vs_ai_faces/dataset/train/fake',\n",
    "        'datasets/real_vs_ai_faces/dataset/val/fake',\n",
    "        'datasets/real_vs_ai_faces/dataset/test/fake',\n",
    "    ]\n",
    "\n",
    "    # Copy function with filename collision handling\n",
    "    def copy_file(src_path, dst_folder):\n",
    "        filename = os.path.basename(src_path)\n",
    "        dst_path = os.path.join(dst_folder, filename)\n",
    "\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        counter = 1\n",
    "        while os.path.exists(dst_path):\n",
    "            dst_path = os.path.join(dst_folder, f\"{base}_{counter}{ext}\")\n",
    "            counter += 1\n",
    "\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    # Process each folder\n",
    "    def process_folder(folder, class_type):\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"Folder not found: {folder}\")\n",
    "            return\n",
    "\n",
    "        dst_folder = output_real if class_type == 'real' else output_fake\n",
    "        images = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            list(tqdm(executor.map(lambda img: copy_file(img, dst_folder), images),\n",
    "                      total=len(images),\n",
    "                      desc=f'Copying {class_type} from {os.path.basename(folder)}',\n",
    "                      unit='img'))\n",
    "\n",
    "    # Process all folders\n",
    "    for folder in input_dirs:\n",
    "        if 'real' in folder:\n",
    "            process_folder(folder, 'real')\n",
    "        elif 'fake' in folder:\n",
    "            process_folder(folder, 'fake')\n",
    "\n",
    "    print(\"\\nAll images successfully combined into 'data/real' and 'data/fake'.\")\n",
    "\n",
    "prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def balance_dataset():\n",
    "    \"\"\"Balance the dataset by moving extra files\"\"\"\n",
    "    def move_extras(src_folder, max_keep, dest_folder):\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        files = [f for f in os.listdir(src_folder) if os.path.isfile(os.path.join(src_folder, f))]\n",
    "        \n",
    "        if len(files) <= max_keep:\n",
    "            print(f\"No extra files to move from {src_folder}. Total: {len(files)}\")\n",
    "            return\n",
    "\n",
    "        extra_files = random.sample(files, len(files) - max_keep)\n",
    "\n",
    "        for f in tqdm(extra_files, desc=f\"Moving extras from {src_folder} to {dest_folder}\", unit=\"file\"):\n",
    "            src_path = os.path.join(src_folder, f)\n",
    "            dest_path = os.path.join(dest_folder, f)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "        print(f\"Moved {len(extra_files)} files to {dest_folder}\")\n",
    "\n",
    "    # Balance to 5000 images per class\n",
    "    move_extras(\"data/real\", max_keep=5000, dest_folder=\"extra/real\")\n",
    "    move_extras(\"data/fake\", max_keep=5000, dest_folder=\"extra/fake\")\n",
    "\n",
    "balance_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/real: 5000 files, 1.29 GB (1319.19 MB)\n",
      "data/fake: 5000 files, 6.16 GB (6302.88 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_dataset_stats():\n",
    "    \"\"\"Calculate and display dataset statistics\"\"\"\n",
    "    def count_files_and_size(folder):\n",
    "        total_size = 0\n",
    "        total_files = 0\n",
    "        for root, _, files in os.walk(folder):\n",
    "            total_files += len(files)\n",
    "            for f in files:\n",
    "                fp = os.path.join(root, f)\n",
    "                if os.path.isfile(fp):\n",
    "                    total_size += os.path.getsize(fp)\n",
    "        size_mb = total_size / (1024 * 1024)\n",
    "        size_gb = total_size / (1024 * 1024 * 1024)\n",
    "        return total_files, size_mb, size_gb\n",
    "\n",
    "    # Folder paths\n",
    "    real_path = 'data/real'\n",
    "    fake_path = 'data/fake'\n",
    "\n",
    "    # Count files and sizes\n",
    "    real_files, real_mb, real_gb = count_files_and_size(real_path)\n",
    "    fake_files, fake_mb, fake_gb = count_files_and_size(fake_path)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"data/real: {real_files} files, {real_gb:.2f} GB ({real_mb:.2f} MB)\")\n",
    "    print(f\"data/fake: {fake_files} files, {fake_gb:.2f} GB ({fake_mb:.2f} MB)\")\n",
    "\n",
    "get_dataset_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
